## KNN 知识点总结

[TOC]

### 1、QA

1. **KNN 算法的一些关键假设是什么？它们如何影响其在不同场景下的性能？**

    K近邻（KNN）是一种用于分类和回归任务的机器学习算法。该算法通过在特征空间中找到 $k$ 个最近的训练示例，并基于其 $k$ 个最近邻居的标签或值来分类或预测测试示例的标签来工作。

    以下是 KNN 算法的一些关键假设：

    1. **数据被归一化**：KNN 算法假设数据被归一化。归一化意味着特征值的范围应该是相似的，因此没有一个特征比其他特征具有更大的权重。由于某些特征可能在距离计算中占主导地位，因此未能对数据进行归一化可能会导致错误分类。
    2. **距离度量：**KNN 算法依赖于距离度量来度量实例之间的相似度。该算法假设所使用的距离度量适用于当前的问题。例如，欧几里德距离对于具有不相关特征的高维数据可能不适用。
    3. **K 值：**KNN 算法假设已选择正确的 $K$ 值。$K$ 的值决定了用于分类或预测标签的邻居的数量。$K$ 值小可能会导致过拟合，而 $K$ 值大可能会导致欠拟合。
    4. **数据稀疏性：** KNN 算法可能无法很好地处理稀疏数据。在稀疏数据集中，大多数特征的值为零，这可能导致不正确的距离计算。

    这些假设对 KNN 性能的影响可能因具体问题和数据集而异。例如，使用不适当的距离度量可能导致不正确的分类，而 $K$ 值较小可能导致过拟合，$K$ 值较大可能导致欠拟合。此外，KNN 可能无法很好地处理稀疏数据或未标准化的数据。因此，必须仔细选择适当的距离度量、K 值和预处理技术，以确保 KNN 算法对给定数据集和问题的最佳性能。

2. **KNN 中使用的一些最常见的距离度量有哪些？什么时候会选择其中一个而不是另一个？**

    1. **欧几里得距离**：这是 KNN 中最常用的距离度量。它测量多维空间中两点之间的直线距离。它适用于特征具有相同测量单位且特征值不是分类的数据集。
    2. **曼哈顿距离：**该距离度量通过将两个点沿每个维度的坐标之间的绝对差值相加来测量两个点之间的距离。它适用于特征具有不同测量单位或分类的数据集。
    3. **切比雪夫距离：**该距离度量度量沿任意维度两点之间的最大距离。它适用于某些维度上可能存在异常值的数据集，并且需要对异常值赋予更多权重。
    4. **闵可夫斯基距离：**这是欧几里德距离和曼哈顿距离的推广，其中参数 $p$ 用于控制对大距离或小距离的强调程度。当 $p=1$ 时，它减少到曼哈顿距离，当 $p=2$ 时，它减小到欧几里德距离。

    在 KNN 中选择距离度量时，必须考虑数据集的性质和当前的问题。例如，欧几里德距离适用于特征具有相同测量单位且特征值不是分类的数据集，而曼哈顿距离适用于具有不同测量单位或分类的特征的数据集。切比雪夫距离在某些维度上的异常值需要给予更多权重时是合适的。闵可夫斯基距离可用于控制对大距离或小距离的强调程度。

    （**更多距离度量方法，参见：[度量方法总结](https://github.com/dfsj66011/KB/blob/main/ML%26DL/distance%20measurement.md)**）

3. **在什么情况下，维度诅咒可能会影响 KNN 算法的性能，如何解决这个问题？**

    维度诅咒是指特征空间中的维度增加导致数据点稀疏的问题，使得使用 KNN 等机器学习算法进行准确预测或分类变得更加困难。**当数据集中的特征（维度）数量相对于样本数量较大时，KNN 算法中可能会出现维度诅咒**。

    维度的诅咒可以以下列方式影响 KNN 算法的性能：

    1. **计算复杂度增加**：随着维数的增加，获得代表性样本所需的数据点数量呈指数级增加，这使得算法的计算成本增加。
    2. **邻居之间的距离增加**：随着维度数量的增加，邻居之间的间距变得不那么有意义，许多邻居可能具有相似的距离。这可能导致 KNN 算法无法准确地对新数据点进行分类。

    为了解决 KNN 中的维度诅咒，可以使用以下技术：

    1. **特征选择**：这包括选择对分类或预测贡献最大的最重要特征的子集。这降低了数据的维度，有助于减轻维度的诅咒。
    2. **特征提取**：这涉及将数据转换为低维空间，同时保留最重要的信息。这可以通过主成分分析（PCA）等技术实现。
    3. **局部敏感哈希（LSH）**：这是一种技术，用于将高维数据点映射到低维哈希代码，同时保持点之间的距离。这降低了数据的维数，同时保持了 KNN 算法的准确性。
    4. **核方法**：这涉及将数据映射到更高维度的特征空间，在那里可以更好地分离数据点，而无需显式计算更高维度表示。该技术有助于解决 KNN 中的维数问题。

    总之，维数的诅咒会影响 KNN 算法的性能，并且可以使用诸如特征选择、特征提取、局部敏感哈希和核方法等技术来缓解这个问题。技术的选择应取决于具体问题和数据集。

4. **如何选择 KNN 中 $k$ 参数的最佳值，以及哪些因素可能会影响此决定？**

    选择 KNN 中 $k$ 参数的最佳值是建立精确 KNN 模型的重要步骤。$k$ 参数是算法在进行预测时将考虑的最近邻居的数量。为 $k$ 选择合适的值取决于几个因素，例如数据集的大小、特征的数量以及使用的距离度量。

    以下是选择 $k$ 的最佳值的一些步骤：

    1. 将数据集拆分为训练集和验证集：为了评估 KNN 算法的性能，将数据集划分为训练集与验证集非常重要。训练集用于训练模型，而验证集用于评估模型的性能。
    2. 定义 $k$ 值范围：定义要测试的 $k$ 值范围。通常，将从一个较小的 $k$ 值开始，例如 1，然后逐渐增加 $k$ 值，直到达到模型精度开始降低的点。
    3. 训练 KNN 模型：使用训练集和不同的 $k$ 值训练 KNN 模型。
    4. 评估模型的性能：使用适当的性能指标（如准确度、精度或召回率）评估模型在验证集上的性能。
    5. 选择最佳 $k$ 值：选择在验证集上提供最佳性能的 $k$ 值。

    可能影响最佳 $k$ 值选择的因素包括：

    1. 数据集大小：对于小数据集，较小的 $k$ 值可能更合适，而对于较大的数据集，可能需要较大的 $k$ 值才能获得良好的性能。
    2. 数据稀疏性：如果数据集是稀疏的，较小的 $k$ 值可能更适合避免过度拟合。
    3. 类不平衡：如果数据集不平衡，较小的 $k$ 值可能更适合确保算法考虑更多的少数类。
    4. 距离度量：最佳 $k$ 值可能会根据使用的距离度量而变化。例如，对于曼哈顿距离，较小的 $k$ 值可能比欧几里德距离更合适。

    总之，在 KNN 中选择 $k$ 的最佳值包括将数据集分成训练集和验证集，定义要测试的 $k$ 值范围，训练模型，评估其性能，以及选择在验证集上提供最佳性能的 $k$ 值。数据集大小、数据稀疏性、类不平衡和距离度量等因素会影响最佳 $k$ 值的选择。

5. **KNN 如何处理不平衡的数据集，在这种情况下可以使用哪些技术来提高其性能？**

    KNN 对不平衡的数据集很敏感，其中一个类中的示例数量远远超过另一个类的示例数量。在这种情况下，KNN 倾向于支持多数类，导致少数类表现不佳。以下是一些可用于提高 KNN 在不平衡数据集上的性能的技术：

    1. 欠采样：欠采样包括从多数类中随机移除示例，以平衡每个类中的示例数量。这有助于减少对多数类的偏见。
    2. 过采样：过采样涉及为少数类创建合成示例，以增加该类中的示例数量。这有助于提高少数群体的代表性，减少对多数群体的偏见。过采样的常用技术包括合成少数过采样技术（SMOTE）和自适应合成采样（ADASYN）。
    3. 加权 KNN：加权 KNN 为少数群体中的示例分配更高的权重，这有助于平衡多数群体和少数群体的影响。
    4. 基于距离的采样：基于距离的抽样涉及从少数类中选择与多数类中的示例最接近的示例。这有助于确保少数群体中的例子得到充分代表，并减少对多数群体的偏见。
    5. 集成方法：集成方法包括组合在不同数据子集上训练的多个 KNN 模型。这有助于提高少数群体的代表性，减少对多数群体的偏见。
    6. 调整决策阈值：默认情况下，KNN 使用 0.5 决策阈值对示例进行分类。调整这个阈值有助于提高模型对少数群体的敏感性。

    总之，KNN 可以对不平衡数据集敏感，但可以使用欠采样、过采样、加权 KNN、基于距离的采样、集成方法和调整决策阈值等技术来提高其在此类数据集上的性能。技术的选择取决于问题和数据集的具体情况。

6. **KNN 在处理有噪声或缺失的数据时表现如何，可以使用哪些预处理技术来缓解这些问题？**

    KNN 对噪声或缺失的数据很敏感，因为它依赖于实例之间的距离来进行预测。噪声或缺失的数据会扭曲距离测量结果，并导致错误的预测。以下是一些可用于缓解这些问题的预处理技术：

    1. **异常值检测和删除**：异常值是与其他数据显著不同的数据点。它们可能由测量错误、数据输入错误或其他异常引起。可以使用统计方法（如 z-score 或四分位间距（IQR））检测异常值，并将其从数据集中删除。
    2. **数据插补**：缺失数据可以使用各种技术进行插补，如均值插补、模式插补或回归插补。均值插补包括用特征的均值替换缺失值。模式插补涉及用特征的最常见值替换缺失值。回归插补涉及使用回归模型来预测缺失值。
    3. **特征缩放**：特征缩放涉及将特征值缩放到类似范围。这有助于避免具有较大值的特征对距离测量的影响。常用的缩放方法包括最小-最大缩放和标准化。
    4. **降维**：可以使用主成分分析（PCA）或 t-分布随机邻居嵌入（t-SNE）等技术将高维数据降到较低维空间。这有助于去除不相关或冗余的特征，并提高 KNN 模型的准确性。
    5. **稳健的距离度量**：距离度量（如 Mahalanobis 距离或 Manhattan 距离）比欧氏距离对有噪声的数据更稳健。这些距离度量分别考虑了协方差矩阵或特征之间的绝对差异，并且可能对异常值不太敏感。

    总之，KNN 可能对噪声或缺失数据敏感，但可以使用异常值检测和去除、数据插补、特征缩放、降维和稳健距离度量等技术来缓解这些问题。技术的选择取决于问题和数据集的具体情况。

7. **与其他分类算法相比，KNN 的一些局限性是什么？什么时候可以选择使用一种而不是另一种？**

    虽然 KNN 是一种简单直观的分类算法，但与其他分类算法相比，它有一些局限性。以下是 KNN 的一些限制：

    1. 计算成本高昂：KNN 的计算成本高昂，尤其是对于大型数据集或高维数据。随着示例和特征的数量增加，搜索 $k$ 个最近邻居所需的时间也增加。
    2. 对距离度量的选择敏感：KNN 的性能在很大程度上取决于距离度量的选项。不同的距离度量可能更适合不同类型的数据，并且确定给定问题的最佳距离度量可能具有挑战性。
    3. 需要仔细的预处理：KNN 可能对噪声或丢失的数据以及不相关或冗余的特征敏感。为了提高 KNN 模型的准确性，可能需要预处理步骤，如异常值检测、数据插补、特征缩放和维度缩减。
    4. 难以处理分类特征：KNN  最适合处理数字数据，并且处理分类特征可能具有挑战性。一种热编码可用于将分类特征转换为数字特征，但这会增加数据的维数，并使 KNN 模型的计算成本更高。

    在决定是否使用 KNN 或其他分类算法时，请考虑以下因素：

    1. 数据集大小：对于大型数据集，KNN 的计算成本可能很高。如果数据集非常大，其他算法（如逻辑回归或决策树）可能更合适。
    2. 特征类型：KNN 最适合数字数据。如果数据集包含分类特征或文本数据，则决策树或支持向量机等其他算法可能更合适。
    3. 距离度量：距离度量的选择会显著影响 KNN 的性能。如果无法为数据找到合适的距离度量，则其他算法可能更合适。
    4. 类不平衡：KNN 对阶级不平衡很敏感。如果数据集高度不平衡，其他算法（如随机森林或具有类权重的支持向量机）可能更合适。

    总之，与其他分类算法相比，KNN 具有一些局限性，包括其计算复杂性、对距离度量选择的敏感性以及难以处理分类特征。在 KNN 和其他分类算法之间进行选择时，请考虑数据集的大小、特征类型、距离度量的选择以及类别不平衡的程度。

8. **KNN 算法的一些最常见的改进，例如加权 KNN 或核密度估计，以及它们如何影响其性能？**

    1. **加权 KNN**：在标准 KNN 中，每个邻居对预测的贡献相等。在加权 KNN 中，每个邻居的贡献由其与查询点的距离加权。更接近查询点的邻居具有更高的权重，并且对预测具有更大的影响。加权 KNN 可以提高算法的准确性，特别是对于类不平衡的数据集。
    2. **核密度估计**：核密度估计（KDE）是一种非参数技术，用于估计随机变量的概率密度函数。在 KNN 的上下文中，KDE 可用于估计给定查询点处每个类的概率密度。预测等级是密度最高的等级。KDE 可以提高 KNN 的准确性，特别是对于具有重叠类的数据集。
    3. **局部异常值因子**：局部异常值因素（LOF）是一种检测高维数据集中异常值的技术。LOF 可以在 KNN 中用于基于其局部密度为每个邻居分配权重。密集区域中的邻居具有更高的权重，对预测的影响更大。LOF 可以提高 KNN 的准确性，特别是对于具有高维数据和异常值的数据集。
    4. **基于半径（Radius-based）的 KNN**：在标准 KNN 中，邻居的数量是固定的。在基于半径的 KNN 中，邻居的数量由查询点周围的固定半径确定。仅使用半径内的邻居进行预测。基于半径的 KNN 可以提高算法的效率，减少有噪声或不相关邻居的影响。
    5. **增量 KNN**：增量 KNN 是 KNN 的一种变体，可以处理增量学习。每次添加新数据时，增量 KNN 都会用新数据更新现有模型，而不是从头开始重建模型。对于不断变化的数据集，增量 KNN 比标准 KNN 更有效。

    这些改进会以各种方式影响 KNN 的性能。加权 KNN 和 KDE 可以提高 KNN 的准确性，特别是对于类不平衡或类重叠的数据集。LOF 可以提高 KNN 的准确性，特别是对于具有高维数据和异常值的数据集。基于半径的 KNN 可以提高算法的效率，减少有噪声或不相关邻居的影响。对于不断变化的数据集，增量 KNN 比标准 KNN 更有效。修改的选择取决于问题和数据集的具体情况。

9. **如何评估 KNN 模型的性能，以及使用哪些度量来衡量其准确性和泛化能力？**

    为了评估 KNN 模型的性能，可以使用各种度量来衡量其准确性和泛化能力。以下是一些常用的评估指标：

    1. 准确度：最简单和最常用的度量是准确度，它衡量模型做出正确预测的比例。它被计算为正确预测数除以预测总数。
    2. 混淆矩阵：混淆矩阵是一个表，汇总了每个类的真阳性、真阴性、假阳性和假阴性的数量。它可以提供关于模型性能的更详细信息，而不仅仅是精度。
    3. 精确度和召回率：准确度衡量的是阳性预测总数中的真阳性比例，而召回率衡量的是实际阳性总数中的真实阳性比例。当处理不平衡的数据集时，精度和召回是有用的指标，其中每个类中的样本数量不相等。
    4. F1-score: F1 分数是准确度和召回率的调和平均值。它提供了一个平衡两个指标的单一分数，在类不平衡时特别有用。
    5. ROC 曲线和 AUC：ROC 曲线是不同阈值下真阳性率与假阳性率的曲线图。ROC 曲线下面积（AUC）衡量模型在所有可能阈值上的整体性能。ROC 曲线和 AUC 是处理二进制分类问题时有用的度量。
    6. 交叉验证：交叉验证是一种评估模型泛化能力的技术。它包括将数据集拆分为训练集和验证集，并在数据的不同子集上重复训练和评估模型。交叉验证有助于估计模型对未发现数据的预期性能，并检测过度拟合。

    评估指标的选择取决于问题和数据集的具体情况。准确度、精度和召回率对于分类问题很有用，而均方误差和 R 平方对于回归问题很有用。交叉验证对于估计模型的泛化能力是有用的，而 ROC 曲线和 AUC 对于二元分类问题是有用的。

10. **如何从头开始实施 KNN，使用哪些库或框架来优化其在生产环境中的性能和可扩展性？**

    从头开始实现 KNN 涉及使用 Python 等编程语言编写算法代码。以下是所涉及步骤的高级概述：

    1. 加载数据集：使用数据加载库（如 Pandas 或 NumPy）将数据集加载到内存中。
    2. 拆分数据集：使用 scikit-learn 或 TensorFlow 等库将数据集拆分为训练集和测试集。
    3. 实现 KNN 算法：编写 KNN 算法代码，包括计算样本之间的距离和选择 $K$ 个最近的邻居。
    4. 进行预测：使用 $K$ 个最近的邻居对测试集中的每个样本进行预测。
    5. 评估模型：使用准确性、准确性、召回率和 F1分数等评估指标来评估模型的性能。

    为了在生产环境中优化 KNN 的性能和可扩展性，可以使用专为高性能计算和分布式计算而设计的库和框架。一些用于优化 KNN 的流行库和框架包括：

    1. Scikit-learn: Scikit-learn 是一个流行的 Python 机器学习库，包括许多算法的实现，包括 KNN。它针对性能进行了优化，并为数据预处理、模型选择和评估提供了许多功能。
    2. Apache Spark: Apache Spark 是一个分布式计算框架，它支持机器学习算法，包括 KNN。它可以用于将 KNN 扩展到跨机器集群的大型数据集。
    3. Dask: Dask 是一个并行计算框架，可用于通过利用多个内核或多台机器将 KNN 扩展到大型数据集。

    在生产设置中实现 KNN 时，必须考虑内存使用、计算效率和可扩展性等因素。使用优化的库和框架可以帮助解决这些问题，并确保算法可以处理大型数据集和高吞吐量应用程序。

11. **如何将 KNN 用于回归问题，以及需要对算法进行哪些修改以确保准确预测？**

    KNN 也可以用于回归问题，通过修改算法来预测连续输出变量而不是分类输出变量。在 KNN 回归中，新数据点的预测输出是训练数据中其 $k$ 个最近邻居的输出值的平均值。要使用 KNN 进行回归，我们需要对算法进行以下修改：

    1. 使用适用于连续变量的距离度量，例如欧几里德距离或曼哈顿距离。
    2. 不要使用 $k$ 个最近邻居的多数类标签，而是计算它们的输出值的平均值。
    3. 使用不同的误差度量来评估模型的性能，例如均方误差或平均绝对误差。
    4. 通过比较不同 $k$ 值的模型性能，使用交叉验证确定最佳 $k$ 值。
    5. 规范化或标准化特征，以确保所有特征对距离度量的贡献相等。

    一般来说，KNN 回归最适合具有少量输入特征的数据集，其中特征和输出变量之间的关系并不太复杂。对于具有许多特征或复杂关系的数据集，线性回归、决策树或神经网络等其他回归算法可能更适合。

12. **在处理大型数据集或高维数据时用于加快 KNN 算法的一些最常见的优化技术**

    KNN 是一种相对简单和直接的算法，但在处理大型数据集或高维数据时，它的计算成本可能很高。以下是用于加快 KNN 的一些常见优化技术：

    1. **维数减少**：随着特征或维数的增加，数据点之间的距离变得不那么有意义，算法也变得效率降低。可以使用诸如主成分分析（PCA）或t-分布随机邻居嵌入（t-SNE）之类的降维技术来减少维数，同时尽可能多地保留原始信息。
    2. **近似最近邻**：对于大型数据集，精确计算所有数据点对之间的距离可能会很昂贵。近似最近邻算法，例如位置敏感散列（LSH）或分层可导航小世界图（HNSW）可以用于以低得多的计算成本找到近似最近邻。
    3. **数据采样**：在某些情况下，可以通过随机采样数据子集来减小数据集的大小。这可以显著降低算法的计算成本，同时仍然提供准确的结果。
    4. **算法优化**：基本 KNN 算法可以通过各种方式进行优化，以降低计算成本，例如使用 KD 树或 ball 树等数据结构来组织数据点并加快搜索最近邻居。
    5. **并行化**：KNN 是一种高度可并行化的算法，可以在多个处理器或机器上运行，以加快计算速度。可以使用 MPI 或 ApacheSpark 等库实现并行化。
    6. **近似 KNN**：对于非常大的数据集或高维数据，可以使用近似 KNN 算法（如近似 KNN（AKNN）或快速 KNN（FKNN））以更低的计算成本找到近似最近邻居。这些算法牺牲了一些精度以换取速度，但在许多情况下仍然可以提供良好的结果。

    总体而言，优化技术的选择将取决于特定的数据集和可用的计算资源。

13. **KNN中使用的一些最常见的正则化技术吗？它们如何影响算法推广到新数据的能力？**

    KNN 是一种非参数算法，这意味着它没有任何可以调整以控制其复杂性或防止过度拟合的显式模型参数。然而，有几种技术可用于正则化 KNN 算法，并提高其推广到新数据的能力。以下是 KNN 中使用的一些最常见的正则化技术：

    1. 距离加权：距离加权是一种技术，用于为距离查询点更近的数据点赋予更多权重。这有助于减少远距离数据点的影响，提高算法的泛化能力。可以使用不同的权重函数，例如逆距离加权或高斯加权。
    2. 特征选择：特征选择是选择与分类任务最相关的可用特征子集的过程。这有助于减少数据的维数，并通过关注信息量最大的特征来防止过度拟合。
    3. 交叉验证：交叉验证是一种技术，用于通过将可用数据分成训练集和验证集来评估算法对新数据的性能。这有助于识别过度拟合并确定k参数的最佳值。
    4. $k$ 的正则化：$k$ 参数的正则化可以通过设置k的最大值或使用基于数据点的局部密度调整k值的自适应 $k$ 方法来完成。这可以通过避免使用太多邻居来帮助防止过度拟合。
    5. 局部平均：局部平均是一种通过对查询点周围局部区域的分类结果进行平均来平滑 KNN 算法的决策边界的技术。这有助于减少噪声和异常值对分类结果的影响。

    总的来说，正则化技术可以通过减少过度拟合和关注信息量最大的数据点和特征来帮助提高KNN算法的泛化性能。技术的选择将取决于特定的数据集和分类任务的性质。

14. **如何使用网格搜索、随机搜索或其他优化技术调整 KNN 模型的超参数，以及需要注意的一些陷阱？**

    要调整 KNN 模型的超参数，可以使用网格搜索、随机搜索或贝叶斯优化等技术。以下是这些技术的简要概述：

    1. 网格搜索：网格搜索涉及使用网格中指定的不同超参数组合来评估 KNN 模型的性能。该技术详尽地搜索所有可能的超参数组合以找到最佳集合。然而，网格搜索在计算上可能很昂贵，尤其是在处理大量超参数或大型数据集时。
    2. 随机搜索：随机搜索包括从指定分布中随机采样超参数，然后使用这些超参数评估 KNN 模型的性能。该技术可以比网格搜索更有效，因为它不会彻底搜索超参数的所有可能组合。然而，它可能仍然需要大量的迭代来找到最佳超参数。
    3. 贝叶斯优化：贝叶斯优化涉及构建目标函数的概率模型，并使用该模型选择下一组要评估的超参数。这种技术比随机搜索更有效，因为它可以基于先前的评估智能地选择超参数，但它可能需要更多的计算资源来设置和运行。

    使用这些技术时，务必注意以下陷阱：

    1. 过拟合：当调整超参数时，可能会将模型过度拟合到训练数据，从而导致新数据的泛化能力较差。为了避免这种情况，在超参数调整期间，使用验证集来评估模型的性能非常重要。
    2. 数据泄漏：确保在不用于模型训练的单独验证集上执行超参数调整非常重要。否则，可以基于训练数据来调整超参数，从而导致过度拟合。
    3. 计算资源：网格搜索和贝叶斯优化在计算上非常昂贵，尤其是在处理大量超参数或大型数据集时。可能需要使用并行计算或分布式计算来加速该过程。
    4. 可解释性：当使用优化技术调整超参数时，很难解释结果并理解超参数如何影响模型的性能。可能需要执行额外的分析，以深入了解模型的行为。



### 2、基础知识

1. k 近邻法是基本且简单的分类与回归方法。k 近邻的**基本做法**是：对给定的训练实例点和输入实例点，首先确定输入实例点的 k 个最近邻训练实例点，然后利用这 k 个训练实例点的类的多数来预测输入实例点的类。
2. k 近邻模型对应于基于训练数据集对特征空间的一个划分。k 近邻法中，当训练集、距离度量、k 值及分类决策规则确定后，其结果**唯一**确定。
3. k 近邻法三要素：距离度量、k 值的选择和分类决策规则。
    * 常用的距离度量是欧式距离：$L_2(x_i,x_j)=\left( \sum^n_{t=1}|x_i^{(l)}-x_j^{(l)}|^2\right)^{\frac{1}{2}}$ 及更一般的 $L_p$ 距离（也叫切比雪夫距离） $L_{\infty}(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|$
    * k 值小时，k 近邻模型更复杂；k 值大时，k 近邻模型更简单。k 值的选择反映了对近似误差与估计误差之间的权衡，通常由检查验证选择最优的 k。
    * 常用的分类决策规则是多数表决，对应于经验风险最小化。误分类率为：$\frac{1}{k}\sum_{x_i \in N_k(x)}I(y_i \neq c_j)=1-\frac{1}{k}\sum_{x_i \in N_k(x)}I(y_i=c_j)$，其最近邻的 k 个训练实例点构成的集合为 $N_k(x)$，涵盖该区域的类别是 $c_j$。
4. k 近邻法的实现需要考虑如何快速搜索 k  个最近邻点。kd 树是一种便于对 k 维空间中的数据进行快速检索的数据结构。kd 树是二叉树，表示对 k 维空间的一个划分，其每个结点对应于 k 未空间划分中的一个超矩形区域。利用 kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量。
5. K 的初始值可以选择样本数的均方根，K 值的选择一般倾向于奇数。

---

### 2、KNN 代码

算法伪代码：

```
KNN 没有显式的学习过程，直接从推理开始搞：

1. 加载数据
2. 初始化 k 值
3. 为了得到预测 class，在所有的训练数据点上进行：
    1. 计算测试数据与每个训练数据之间的距离。这里我们将使用欧几里德距离作为我们的距离度量，因为它是最流行的方法。其他可以使用的指标有切比雪夫、余弦等。
    2. 根据距离值按升序排列计算的距离
    3. 从排序的数组中获取 top K
    4. 获取这些类别中最频繁的类
    5. 返回预测结果
```

下面是一个使用 scikit-learn 库在 Python 中使用 KD 树实现 KNN 的示例：

```python
from sklearn.neighbors import KDTree, KNeighborsClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# Load the iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Construct a KD tree with the training data
tree = KDTree(X_train)

# Create a KNN classifier with k=3 and KD tree search
knn = KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree')

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Predict the class labels for the test data
y_pred = knn.predict(X_test)

# Evaluate the accuracy of the classifier
accuracy = knn.score(X_test, y_test)
print("Accuracy:", accuracy)

```

---

### 3、练习题

1、**k-NN 算法测试时间的计算量大于训练时间的计算量。**

答案：True。该算法的训练阶段只需要存储训练样本的特征向量和类标签。在测试阶段，通过分配最接近该查询点的 k 个训练样本中最频繁的标签对测试点进行分类，因此计算量更高。



2、**根据下图，K 值选择多少比较合适？**

<img src="https://www.analyticsvidhya.com/wp-content/uploads/2014/10/training-error_11.png" width=500>

答案：10。当 k 值为 10 时，验证误差最小，所以最好使用这个 k 值。



3、**下面哪种距离度量方式不能用于 KNN 中？**

A. 哈曼顿（Manhattan）：$L_1(x_i,x_j)=\left( \sum^n_{t=1}|x_i^{(l)}-x_j^{(l)}|\right)$ 

B. 闵可夫斯基（Minkowski）：$L_p(x_i,x_j)=\left( \sum^n_{t=1}|x_i^{(l)}-x_j^{(l)}|^p\right)^{\frac{1}{p}}$ 

C. Jaccard 距离

D. Tanimoto 距离

E. 马氏（mahalanobis）距离：$D_{M}(x, y)=\sqrt{(x-y)^{T} \Sigma^{-1}(x-y)}$

F. 以上均可

答案：F。虽然都可以，但是最常用的仍然是欧式距离。



4、**KNN 是否可用于回归问题？**

答案：可以的。在这种情况下，预测可以基于 k-最相似实例的平均值或中位数。



5、**关于 KNN 下列哪些说法是正确的？**

* 如果所有数据具有相同的尺度，k-NN 的性能会更好
* k-NN 对于少量的输入变量（p）工作得很好，但是当输入的数量非常大时却很困难
* k-NN 对所解问题的函数形式不作任何假设

答案：以上说法均正确。



6、**以下哪种算法可以用来插补分类变量和连续变量的缺失值？**

A. KNN   B. 线性回归     C. 逻辑回归

答案：A



7、**关于曼哈顿距离，说法正确的是？**

A. 可以用来处理连续变量

B. 可以用来处理分类变量

C. 连续变量与分类变量都可以

D. 都不对

答案：A。曼哈顿距离用来计算实值特征之间的距离



8、**下面哪些距离度量方式可以用在 KNN 中解决分类变量？**

* 海明距离
* 欧氏距离
* 曼哈顿距离

答案：只有海明距离可以，欧式距离和曼哈顿距离都是解决连续型变量的。



9、**关于 KNN 中的 “Bias” 说法正确的是：**

* 当增加 k 时，偏差会增加
* 当减少 k 时，偏差会增加

答案：第一个。K 越大，模型越简单，简单的模型具有较高的偏差

 

10、**关于 KNN 中的 variance 说法正确的是：**

* 当增加 k 时，方差会增加
* 当减少 k 时，方差会增加

答案：第二个。简单的模型具有较小的方差。



11、**如果在 KNN 算法中你发现数据中含有 noise，你会怎么做？**

* 考虑增加 K 的值
* 考虑减少 K 的值
* 噪音的影响不依赖于 K 的值，无需调整 K

答案：第一个。应该尝试增加 K 值。

 

12、**在 k-NN 中，由于维数灾难，它很可能过拟合。你会考虑下列哪种方法来处理这个问题？**

* 降维
* 特征选择

答案：都可以考虑。

 

13、**关于 KNN，下面说法正确的是：**

* k-NN 是一种基于记忆的方法，它是在我们收集新的训练数据时，分类器立即适应。
* 在最坏情况下，新样本分类的计算复杂度与训练数据集中的样本数呈线性增长。
* 如果 k 的值非常大，我们会将其他类的点包含到邻域中。
* 如果 k 值比较小，则会对噪音数据非常敏感

 答案：都正确。



14、**假设下面 3 幅图为 KNN 算法在不同 K 值下的结果图，假设从左到右图中的 K 值分别是 K1、K2、K3，那么这三个值之间的大小关系可能是？**

[![img](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/08/02162508/Pic_341.jpg)](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/08/02162508/Pic_341.jpg)

* k1 > k2 > k3
* k1 < k2
* k1 = k2 = k3
* 都不对

答案：都不对，应该是 k3 最大，k1 最小。分类边界线约平滑，k 值越大。



15、**关于 KNN 分类器说法正确的是：**

*  K 值越大，分类精度越高
*  较小的 k 值，决策边界更平滑
*  决策边界是线性的
*  KNN 没有要求一个明确的训练步骤

答案：最后一个。关于第一个，并不总是正确的。你应该保证 k 值不太大也不太小；决策边界可能是参差不齐的，如上图。



16、**有没有可能通过 1-NN 分类器实现一个 2-NN 分类器？**

答案：可以的，通过集成方法即可。

 

17、**下面说法正确的是：**

* K 值的选择，我们可以借助交叉验证
* 欧氏距离认为每个特征是同等重要的

答案：都正确



假设，你已经训练了一个 k-NN 模型，现在你想要得到对测试数据的预测。在预测之前，假设你想计算 k-NN 在测试数据上所花费的时间。

注：计算两个数据之间的距离需要 D 时间。

18、**当 N 非常大时，使用 1-NN 所需要的数据大约是？**

* ND
* 2ND
* ND/2
* 都不对

答案：第一个 ND。



19、**1-NN、2-NN、3-NN 所需要的时间关系大概是？**

* 1NN > 2NN > 3NN
* 1NN < 2NN < 3NN
* 1NN ~ 2NN ~ 3NN
* 都不对

答案：第三个，对于任意 K，其训练时间都差不多的。它们之间唯一区别在于取 top K，算 top K 中的最高频，而整个距离计算过程与 K 是无关的。



20、**为什么说 KNN 是非参数算法？**

答案：术语“非参数”是指不对基础数据分布进行任何假设。这些方法在模型中没有任何固定数量的参数。

类似地，在 KNN 中，通过将每个训练案例作为模型的一个参数，模型参数随着训练数据的增长而增长。因此，KNN 是一种非参数算法。



21、**KNN 算法的空间和时间复杂度？**

答案：时间复杂度方面：距离计算步骤需要二次时间复杂度，并且计算距离的排序需要 $O(N \log N)$。总之，我们可以说这个过程是一个 $O(N^3 \log N)$，这是一个非常长的过程。空间复杂度方面：因为它存储了所有成对的距离，并且在机器的内存中进行排序，所以内存也是一个问题。通常，如果我们有非常大的数据集，本地机器会崩溃。



22、**K 值的选择方法？**

没啥好方法可以直接帮你确定，可以做的是：

* 均方根方法
* 交叉验证方法（Elbow Method，肘）
* 领域知识



23、**KNN 的缺点：**

* 大数据集上效果差
* 高维数据上效果差
* 需要特征归一化
* 对异常点和噪音敏感

